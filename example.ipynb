{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from dataset.dataset_utils import get_video_and_audio\n",
    "from dataset.transforms import make_class_grid, quantize_offset\n",
    "from utils.utils import check_if_file_exists_else_download, which_ffmpeg\n",
    "from scripts.train_utils import get_model, get_transforms, prepare_inputs\n",
    "\n",
    "\n",
    "def reencode_video(path, vfps=25, afps=16000, in_size=256):\n",
    "    assert which_ffmpeg() != '', 'Is ffmpeg installed? Check if the conda environment is activated.'\n",
    "    new_path = Path.cwd() / 'vis' / f'{Path(path).stem}_{vfps}fps_{in_size}side_{afps}hz.mp4'\n",
    "    new_path.parent.mkdir(exist_ok=True)\n",
    "    new_path = str(new_path)\n",
    "    cmd = f'{which_ffmpeg()}'\n",
    "    # no info/error printing\n",
    "    cmd += ' -hide_banner -loglevel panic'\n",
    "    cmd += f' -y -i {path}'\n",
    "    # 1) change fps, 2) resize: min(H,W)=MIN_SIDE (vertical vids are supported), 3) change audio framerate\n",
    "    cmd += f\" -vf fps={vfps},scale=iw*{in_size}/'min(iw,ih)':ih*{in_size}/'min(iw,ih)',crop='trunc(iw/2)'*2:'trunc(ih/2)'*2\"\n",
    "    cmd += f\" -ar {afps}\"\n",
    "    cmd += f' {new_path}'\n",
    "    subprocess.call(cmd.split())\n",
    "    cmd = f'{which_ffmpeg()}'\n",
    "    cmd += ' -hide_banner -loglevel panic'\n",
    "    cmd += f' -y -i {new_path}'\n",
    "    cmd += f' -acodec pcm_s16le -ac 1'\n",
    "    cmd += f' {new_path.replace(\".mp4\", \".wav\")}'\n",
    "    subprocess.call(cmd.split())\n",
    "    return new_path\n",
    "\n",
    "\n",
    "def decode_single_video_prediction(off_logits, grid, item):\n",
    "    label = item['targets']['offset_label'].item()\n",
    "    print('Ground Truth offset (sec):', f'{label:.2f} ({quantize_offset(grid, label)[-1].item()})')\n",
    "    print('Prediction Results:')\n",
    "    off_probs = torch.softmax(off_logits, dim=-1)\n",
    "    k = min(off_probs.shape[-1], 5)\n",
    "    topk_logits, topk_preds = torch.topk(off_logits, k)\n",
    "    # remove batch dimension\n",
    "    assert len(topk_logits) == 1, 'batch is larger than 1'\n",
    "    topk_logits = topk_logits[0]\n",
    "    topk_preds = topk_preds[0]\n",
    "    off_logits = off_logits[0]\n",
    "    off_probs = off_probs[0]\n",
    "    for target_hat in topk_preds:\n",
    "        print(\n",
    "            f'p={off_probs[target_hat]:.4f} ({off_logits[target_hat]:.4f}), \"{grid[target_hat]:.2f}\" ({target_hat})')\n",
    "    return off_probs\n",
    "\n",
    "\n",
    "def patch_config(cfg):\n",
    "    # the FE ckpts are already in the model ckpt\n",
    "    cfg.model.params.afeat_extractor.params.ckpt_path = None\n",
    "    cfg.model.params.vfeat_extractor.params.ckpt_path = None\n",
    "    # old checkpoints have different names\n",
    "    cfg.model.params.transformer.target = cfg.model.params.transformer.target\\\n",
    "                                             .replace('.modules.feature_selector.', '.sync_model.')\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfps = 25\n",
    "afps = 16000\n",
    "in_size = 256\n",
    "exp_name = '24-01-04T16-39-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the model\n",
    "cfg_path = f'./logs/sync_models/{exp_name}/cfg-{exp_name}.yaml'\n",
    "ckpt_path = f'./logs/sync_models/{exp_name}/{exp_name}.pt'\n",
    "\n",
    "# if the model does not exist try to download it from the server\n",
    "check_if_file_exists_else_download(cfg_path)\n",
    "check_if_file_exists_else_download(ckpt_path)\n",
    "\n",
    "# load config\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "# patch config\n",
    "cfg = patch_config(cfg)\n",
    "\n",
    "_, model = get_model(cfg, device)\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of items to process. Mind the order: (video_path, offset_sec, v_start_i_sec)\n",
    "to_process = [\n",
    "    ('./data/vggsound/h264_video_25fps_256side_16000hz_aac/3qesirWAGt4_20000_30000.mp4', 1.6, 0.0),\n",
    "    ('./data/vggsound/h264_video_25fps_256side_16000hz_aac/ZYc410CE4Rg_0_10000.mp4', -2.0, 4.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using video: ./data/vggsound/h264_video_25fps_256side_16000hz_aac/3qesirWAGt4_20000_30000.mp4\n",
      "No need to reencode: vfps: 25.0; afps: 16000; min(H, W)=256\n",
      "Ground Truth offset (sec): 1.60 (18)\n",
      "Prediction Results:\n",
      "p=0.8076 (11.5469), \"1.60\" (18)\n",
      "p=0.1760 (10.0234), \"1.80\" (19)\n",
      "p=0.0067 (6.7617), \"-0.40\" (8)\n",
      "p=0.0042 (6.2891), \"1.40\" (17)\n",
      "p=0.0033 (6.0586), \"2.00\" (20)\n",
      "\n",
      "Using video: ./data/vggsound/h264_video_25fps_256side_16000hz_aac/ZYc410CE4Rg_0_10000.mp4\n",
      "No need to reencode: vfps: 25.0; afps: 16000; min(H, W)=256\n",
      "Ground Truth offset (sec): -2.00 (0)\n",
      "Prediction Results:\n",
      "p=0.8291 (12.7734), \"-2.00\" (0)\n",
      "p=0.1194 (10.8359), \"-1.80\" (1)\n",
      "p=0.0419 (9.7891), \"-1.60\" (2)\n",
      "p=0.0072 (8.0234), \"-1.40\" (3)\n",
      "p=0.0013 (6.2969), \"-1.20\" (4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vid_path, offset_sec, v_start_i_sec in to_process:\n",
    "    # (optional) checking if the provided video has the correct frame rates\n",
    "    print(f'Using video: {vid_path}')\n",
    "    v, _, info = torchvision.io.read_video(vid_path, pts_unit='sec')\n",
    "    _, H, W, _ = v.shape\n",
    "    if info['video_fps'] != vfps or info['audio_fps'] != afps or min(H, W) != in_size:\n",
    "        print(f'Reencoding. vfps: {info[\"video_fps\"]} -> {vfps};', end=' ')\n",
    "        print(f'afps: {info[\"audio_fps\"]} -> {afps};', end=' ')\n",
    "        print(f'{(H, W)} -> min(H, W)={in_size}')\n",
    "        vid_path = reencode_video(vid_path, vfps, afps, in_size)\n",
    "    else:\n",
    "        print(\n",
    "            f'No need to reencode: vfps: {info[\"video_fps\"]}; afps: {info[\"audio_fps\"]}; min(H, W)={in_size}')\n",
    "\n",
    "    # load visual and audio streams\n",
    "    # rgb: (Tv, 3, H, W) in [0, 225], audio: (Ta,) in [-1, 1]\n",
    "    rgb, audio, meta = get_video_and_audio(vid_path, get_meta=True)\n",
    "\n",
    "    # making an item (dict) to apply transformations\n",
    "    # NOTE: here is how it works:\n",
    "    # For instance, if the model is trained on 5sec clips, the provided video is 9sec, and `v_start_i_sec=1.3`\n",
    "    # the transform will crop out a 5sec-clip from 1.3 to 6.3 seconds and shift the start of the audio\n",
    "    # track by `offset_sec` seconds. It means that if `offset_sec` > 0, the audio will\n",
    "    # start by `offset_sec` earlier than the rgb track.\n",
    "    # It is a good idea to use something in [-`max_off_sec`, `max_off_sec`] (-2, +2) seconds (see `grid`)\n",
    "    item = dict(\n",
    "        video=rgb, audio=audio, meta=meta, path=vid_path, split='test',\n",
    "        targets={'v_start_i_sec': v_start_i_sec, 'offset_sec': offset_sec, },\n",
    "    )\n",
    "\n",
    "    # making the offset class grid similar to the one used in transforms\n",
    "    max_off_sec = cfg.data.max_off_sec\n",
    "    num_cls = cfg.model.params.transformer.params.off_head_cfg.params.out_features\n",
    "    grid = make_class_grid(-max_off_sec, max_off_sec, num_cls)\n",
    "    if not (min(grid) <= item['targets']['offset_sec'] <= max(grid)):\n",
    "        print(f'WARNING: offset_sec={item[\"targets\"][\"offset_sec\"]} is outside the trained grid: {grid}')\n",
    "\n",
    "    # applying the test-time transform\n",
    "    item = get_transforms(cfg, ['test'])['test'](item)\n",
    "\n",
    "    # prepare inputs for inference\n",
    "    batch = torch.utils.data.default_collate([item])\n",
    "    aud, vid, targets = prepare_inputs(batch, device)\n",
    "\n",
    "    # TODO:\n",
    "    # sanity check: we will take the input to the `model` and recontruct make a video from it.\n",
    "    # Use this check to make sure the input makes sense (audio should be ok but shifted as you specified)\n",
    "    # reconstruct_video_from_input(aud, vid, batch['meta'], vid_path, v_start_i_sec, offset_sec,\n",
    "    #                              vfps, afps)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.set_grad_enabled(False):\n",
    "        with torch.autocast('cuda', enabled=cfg.training.use_half_precision):\n",
    "            _, logits = model(vid, aud)\n",
    "\n",
    "    # simply prints the results of the prediction\n",
    "    decode_single_video_prediction(logits, grid, item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
